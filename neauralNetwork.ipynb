{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Neural Network Fitting Analysis\n",
    "\n",
    "## Preamble\n",
    "\n",
    "The aim of this analysis is to develop a pipeline that consists of a LSTM Neural Network (LSTM NN) that given the opening stock price of a stock over the previous 40 days, can accurately predict the current opening stock price.  This project was inspired by the work done by Yacoub Ahmed that can be found here https://towardsdatascience.com/getting-rich-quick-with-machine-learning-and-stock-market-predictions-696802da94fe.  The aim is while using his original LSTM NN as a starting point, is to improve it and then leverage it to create a simulation of some stock trading on a few automobile stocks.  In the interest of academic honesty, it must be noted that while we took our inspiration from Yacoub and used his original LSTM model, all code used here is original.\n",
    "\n",
    "Note, in call cases where we train a LSTM NN, we have commented it out and replaced it with code that loads in the models from memory.  This is done to save the markers time from having to retrain the models.  Which I certainly do not recommend unless you have a GPU at your disposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yfinance'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-3c752c1b8ecd>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Reading in neccessary packages\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0myfinance\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0myf\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mdatetime\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtimedelta\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'yfinance'"
     ]
    }
   ],
   "source": [
    "# Reading in neccessary packages\n",
    "import yfinance as yf\n",
    "from datetime import date, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/ford.csv', index_col='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Reasonable Predictor\n",
    "\n",
    "To begin, we will build our model on Ford stock from January 1st, 2000 to November 17, 2020 (The day I am creating this).  Lets first as a formality check that the daily opening stock price is actually correlated to the previous days opening stock price.  To check this, we will create a plot of the sample correlation (ACF) below.  If we examine the AF plot below, we can clearly see that the opening price is highly correlated to the previous days opening stock price.  We can actually see this continues for many lags k.  Of course the reader may be wondering what we mean by lag k?  Let the opening stock price of ford be represented by the time series process ${Y_t}$.  The current days opening stock price we are interested in can be denated as $Y_t$.  Then in the plot below we can a high correlation between $Y_t$ and $Y_{t-k}$ where k is some integer k that denotes some number of units back in time.  For instance, from the plot below we can see that the sample ACF between $Y_t$ and $Y_{t-2}$ produces a sample ACF very close to 1.  Moving on, our conclusion from this plot is simply that the current opening stock price of Ford has a high correlation with its past opening stock prices.  Thus, it could be a strong predictor.  Lastly, the question remains how far back in the past to predict the current opening stock price.  That is many many past days should we use to predict the current day.  To choose this we arbitrarilty chose 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CITATION: https://medium.com/@krzysztofdrelczuk/acf-autocorrelation-function-simple-explanation-with-python-example-492484c32711\n",
    "plot_acf(data['Open'], lags=100, title='Sample ACF of Ford Opening Day Stock Price')\n",
    "\n",
    "# NOTE: for some reason this produces 2 of the same plot, it could not be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Code\n",
    "\n",
    "The following few 4 cells contain code that will be use to create, and evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This defines the numpy of days in the past we will use to predict the current price\n",
    "LAG = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the results of a mode.\n",
    "def plot(yHat, y, title):\n",
    "    plt.plot(y)\n",
    "    plt.plot(yHat)\n",
    "    plt.legend(['Real', 'Predicted'])\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the MSE\n",
    "def mse(x, y):\n",
    "    preds = model.predict(x)\n",
    "    preds = scaler.inverse_transform(preds)\n",
    "    unscaled_yTest = scaler.inverse_transform(np.reshape(y, (-1, 1)))\n",
    "    return np.mean(np.square(preds-unscaled_yTest)), preds, unscaled_yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accepts the raw data and transforms it into the form required for the LSTM NN.  \n",
    "# - It removes unneccessary columns\n",
    "# - scales the data between 0 and 1\n",
    "# - transforms into numpy arrays\n",
    "def transform(df, n = 50):\n",
    "    df = pd.DataFrame(df['Open'])    # Removing all the other columns as we are only predicting if we should buy based on the opening stock price\n",
    "\n",
    "    # Normalizing to 0-1 range\n",
    "    scaler = MinMaxScaler(feature_range=(0,1)) \n",
    "    scaler.fit(df)\n",
    "    df = pd.DataFrame(scaler.transform(df),columns=df.columns, index = pd.to_datetime(df.index))\n",
    "    \n",
    "    # Creating 40 columns that give the past 40 day opening stock price\n",
    "    for i in range(1,n+1):\n",
    "        df[f'Open-{i}'] = df['Open'].shift(-i)\n",
    "    \n",
    "    # Subsetting for the neccessary columns\n",
    "    df = df.iloc[0:-n,0:]\n",
    "    \n",
    "    # Splitting into training and testing data (test size is about last 2 years)\n",
    "    dt = pd.to_datetime(date(2020, 11, 17) - timedelta(days=730))\n",
    "    train = df[df.index < dt]\n",
    "    test = df[df.index >= dt]\n",
    "    \n",
    "    # Splitting into appropriat x and y values\n",
    "    xTrain = train.iloc[:,1:]\n",
    "    yTrain = train.iloc[:,0]\n",
    "    xTest = test.iloc[:,1:]\n",
    "    yTest = test.iloc[:,0]\n",
    "    \n",
    "    \n",
    "    # Converting to numpy arrays to feed into model\n",
    "    xTrain = xTrain.to_numpy()\n",
    "    yTrain = yTrain.to_numpy()\n",
    "    xTest = xTest.to_numpy()\n",
    "    yTest = yTest.to_numpy()\n",
    "    \n",
    "    # Reshaping to get correct form\n",
    "    xTrain = np.reshape(xTrain, (xTrain.shape[0], xTrain.shape[1], 1))\n",
    "    xTest = np.reshape(xTest, (xTest.shape[0], xTest.shape[1], 1))\n",
    "    \n",
    "    return xTrain, yTrain, xTest, yTest, scaler    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, yTrain, xTest, yTest, scaler = transform(data, LAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model\n",
    "We have now transformed the data into the required format with both a training and testing.  So lets try fitting to a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CITATION 1: https://www.youtube.com/watch?v=BSpXCRTOLJA\n",
    "# CITATION 2: https://towardsdatascience.com/getting-rich-quick-with-machine-learning-and-stock-market-predictions-696802da94fe\n",
    "# Citation 1 was used as a starting point to get the model to make at least semi-accurate predictions.\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(128,input_shape=(xTrain.shape[1],xTrain.shape[2])))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(64))\n",
    "# model.add(Activation('sigmoid'))\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('linear'))\n",
    "\n",
    "# opt = tf.keras.optimizers.Adam(lr=0.0005)\n",
    "# model.compile(optimizer=opt, loss='mse')\n",
    "# model.fit(xTrain,yTrain, epochs=50, validation_data=(xTest,yTest))\n",
    "\n",
    "# CITATION: https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "# load json and create model\n",
    "json_file = open('data/fordLSTM_50EPOCH.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"data/fordLSTM_50EPOCH.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanSquared, yHat, y = mse(xTrain,yTrain)\n",
    "plot(yHat, y, 'Training Data')\n",
    "print(\"MSE:\",meanSquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(xTest,yTest)\n",
    "meanSquared, yHat, y = mse(xTest,yTest)\n",
    "plot(yHat, y, 'Training Data')\n",
    "print(\"MSE:\",meanSquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the results of our model above.  We can see our model begin to take the rough shape of the actual data on the Test Data.  However, we need it to be far closer.  As we will see shortly, the model is in fact very undertrained.  To adjust for this, lets run the model on varying Epochs (i.e. how many times you run the data through to train the model).  We will test this on 50,100,500,1000 and 2000 epochs and record the MSE to give a rough evaluation of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning 1: Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def epochs(epoch):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(128,input_shape=(xTrain.shape[1],xTrain.shape[2])))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(64))\n",
    "#     model.add(Activation('sigmoid'))\n",
    "#     model.add(Dense(1))\n",
    "#     model.add(Activation('linear'))\n",
    "\n",
    "#     opt = tf.keras.optimizers.Adam(lr=0.0005)\n",
    "#     model.compile(optimizer=opt, loss='mse')\n",
    "#     model.fit(xTrain,yTrain, epochs=epoch, validation_data=(xTest,yTest))\n",
    "#     return model\n",
    "\n",
    "# res = pd.DataFrame(columns=['epoch','mseTest','mseTrain'])\n",
    "# for i in [50,100,500,1000,2000]:\n",
    "#     model = epochs(i)\n",
    "#     meanSquaredTest, yHat, y = mse(xTest,yTest)\n",
    "#     meanSquaredTrain, yHat, y = mse(xTrain,yTrain)\n",
    "#     res = res.append({'epoch':i,'mseTest':meanSquaredTest,'mseTrain':meanSquaredTrain},ignore_index=True)\n",
    "\n",
    "resultsEpoch = pd.read_csv('Data/resultsEpoch.csv')\n",
    "resultsEpoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see the results of training the modle further.  We can clearly see that MSE decreases as we continue to train the model.  However, when we reach 3000 epochs the MSE of the Testing data begins to increase and the MSE of the Training Data begins to decrease.  Is is possible that we are overfitting the data at 3000 epochs.  Thus, we will use 2000 epochs in our model.  Below we vizualize the result of the 2000 epoch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CITATION: https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "json_file = open('data/fordLSTM1.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"data/fordLSTM1.h5\")\n",
    "print(\"Loaded model from disk\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(yHat, y, 'Training Data')\n",
    "meanSquared, yHat, y = mse(xTrain,yTrain)\n",
    "print(\"MSE:\",meanSquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(yHat, y, 'Test Data')\n",
    "meanSquared, yHat, y = mse(xTest,yTest)\n",
    "print(\"MSE:\",meanSquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the plots of our test and training data.  The results appear to be significantly improved considering we have not done any formal tuning at this point.  Surprisingly, we can actually see that that model appears to perform better on the testing data than the training data.  We take this as an indication that the model does not appear to be over fitting the training data.  Thus, no further dropouts will be added to the model nor will any adjustments be made there.  However, we will continue to try and improve the model. \n",
    "\n",
    "Note, the reader may notice that the MSE of the 2000 epoch model displayed with their respective plots differs from the MSE of the 2000 epoch model displayed in the tabel above.  This is likely due to where the MSE's were computed.  While the code was the same, the MSE in the table above was computed on Google Colab while training the models and the MSE computed with the plots was done locally.  There is perhaps some difference of rounding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning 2: Neurons\n",
    "Lets now tune the number of hidden nodes contained in the LSTM layer.  Note, the original value of 128 was chosen arbitrarilty.  To test this, we will do so on a model with 500 epochs to minimize computation time.  Furthermore, not that the range of neurons chosen is in fact arbitraty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epochs(neurons):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons,input_shape=(xTrain.shape[1],xTrain.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=0.0005)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "    model.fit(xTrain,yTrain, epochs=500, validation_data=(xTest,yTest))\n",
    "    return model\n",
    "\n",
    "res = pd.DataFrame(columns=['epoch','mseTest','mseTrain'])\n",
    "for i in [50,70,90,100,120,140]:\n",
    "    model = epochs(i)\n",
    "    meanSquaredTest, yHat, y = mse(xTest,yTest)\n",
    "    meanSquaredTrain, yHat, y = mse(xTrain,yTrain)\n",
    "    res = res.append({'epoch':i,'mseTest':meanSquaredTest,'mseTrain':meanSquaredTrain},ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}